## vLLM
A high-throughput and memory-efficient inference and serving engine for LLMs.

## Tutorial
conda create -n myenv python=3.12 -y \
conda activate myenv \
pip install vllm

## Requirements
OS: Linux
Python version: 3.9 â€“ 3.12 \
GPU: compute capability 7.0 or higher ([V100, A100, RTX 2060 up, etc.](https://developer.nvidia.com/cuda-gpus))  \
Cuda version: offical version above 12.1 (but only above 12.4 is available)

## Original Source
check more from https://github.com/vllm-project/vllm
